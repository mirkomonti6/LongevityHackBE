"""
LangGraph application with suggestion and critic nodes.

This module defines the state schema and graph structure for a simple
two-node workflow: suggestion generation followed by critique.
"""

from typing import TypedDict, Annotated
from typing_extensions import NotRequired
from langgraph.graph import StateGraph, END


class GraphState(TypedDict):
    """
    State schema for the LangGraph application.
    
    This state is passed between nodes and contains:
    - userInput: User's input text
    - messages: Conversation history with role and content
    - pdf: PDF document information (type and data)
    - interviewSummary: Summary from interview node analysis
    - guardrailsStatus: Status message from guardrails check
    - guardrailsSafe: Boolean indicating if content passed safety checks
    - suggestion: The suggestion generated by the suggestion node
    - critique: The critique/evaluation provided by the critic node
    - response: The final response (critique)
    - finalSuggestion: The final suggestion output
    """
    userInput: NotRequired[str]  # User's input text
    messages: NotRequired[list]  # List of message dicts with role/content
    pdf: NotRequired[dict]  # PDF info with type and data fields
    interviewSummary: NotRequired[str]  # Generated by interview node
    guardrailsStatus: NotRequired[str]  # Generated by guardrails node
    guardrailsSafe: NotRequired[bool]  # Safety check result
    suggestion: NotRequired[str]  # Generated by suggestion node
    critique: NotRequired[str]  # Generated by critic node
    response: NotRequired[str]  # Final response (critique)
    finalSuggestion: NotRequired[str]  # Final suggestion output


def extract_pdf_content(pdf: dict) -> str:
    """
    Mock function to simulate PDF content extraction.
    
    In a real implementation, this would extract text from PDF based on the type.
    For now, it returns mocked extracted text.
    
    Args:
        pdf: Dictionary with 'type' and 'data' fields
        
    Returns:
        Mocked extracted text from the PDF
    """
    if not pdf or pdf.get("type") is None:
        return ""
    
    pdf_type = pdf.get("type", "unknown")
    pdf_data = pdf.get("data", "")
    
    # Mock extraction based on type
    if pdf_type == "base64":
        return f"[Mocked PDF extraction] Document provided as base64. Content summary: Research findings on longevity and health optimization."
    elif pdf_type == "url":
        return f"[Mocked PDF extraction] Document from URL: {pdf_data[:50] if pdf_data else 'N/A'}. Content summary: Medical research and clinical data."
    elif pdf_type == "file_id":
        return f"[Mocked PDF extraction] Document with file_id: {pdf_data}. Content summary: Patient health records and analysis."
    else:
        return "[Mocked PDF extraction] Unknown document type."


def suggestion_node(state: GraphState) -> GraphState:
    """
    Node that generates a suggestion.
    
    This node takes the current state and generates a suggestion based on:
    - User input
    - Past conversation messages
    - Extracted PDF content
    
    Args:
        state: The current graph state
        
    Returns:
        Updated state with 'suggestion' and 'finalSuggestion' fields populated
    """
    # Extract input data from state
    user_input = state.get("userInput", "")
    messages = state.get("messages", [])
    pdf = state.get("pdf", {})
    
    # Extract PDF content
    pdf_content = extract_pdf_content(pdf) if pdf else ""
    
    # Build context from messages
    message_context = ""
    if messages:
        message_context = "\n".join([
            f"{msg.get('role', 'unknown')}: {msg.get('content', '')}"
            for msg in messages[-3:]  # Use last 3 messages for context
        ])
    
    # Generate suggestion based on inputs
    # In a real implementation, this would use an LLM
    suggestion_parts = []
    
    if user_input:
        suggestion_parts.append(f"Based on your input: '{user_input}'")
    
    if message_context:
        suggestion_parts.append(f"Considering the conversation context")
    
    if pdf_content:
        suggestion_parts.append(f"and the document content: {pdf_content}")
    
    if suggestion_parts:
        suggestion = f"{' '.join(suggestion_parts)}, I suggest focusing on personalized health optimization strategies that integrate the latest research findings."
    else:
        suggestion = "Consider implementing a feature that improves user engagement through personalized recommendations."
    
    # Return updated state
    return {
        "suggestion": suggestion,
        "finalSuggestion": suggestion  # Both fields have the same value
    }


def critic_node(state: GraphState) -> GraphState:
    """
    Node that critiques/evaluates the suggestion.
    
    This node takes the current state, reads the suggestion,
    and generates a critique or evaluation of that suggestion.
    
    Args:
        state: The current graph state (should contain a 'suggestion' field)
        
    Returns:
        Updated state with 'critique' and 'response' fields populated
    """
    # Get the suggestion from state
    suggestion = state.get("suggestion", "")
    
    # Generate a critique based on the suggestion
    if suggestion:
        critique = f"Critique: The suggestion '{suggestion}' has merit. However, consider the implementation complexity and resource requirements. It would be beneficial to validate user demand before full implementation."
    else:
        critique = "Critique: No suggestion found in state to evaluate."
    
    # Return updated state with both critique and response
    # (response is the output field for the API, critique is for internal use)
    return {
        "critique": critique,
        "response": critique  # Both fields have the same value
    }


def interviewNode(state: GraphState) -> GraphState:
    """
    Node that conducts an interview to gather more information.
    
    This node analyzes the user input and conversation history to determine
    what additional information is needed and formulates follow-up questions.
    
    Args:
        state: The current graph state
        
    Returns:
        Updated state with interview questions or additional context
    """
    # Extract input data from state
    user_input = state.get("userInput", "")
    messages = state.get("messages", [])
    
    # Analyze what information might be missing
    # In a real implementation, this would use an LLM to determine follow-up questions
    interview_notes = []
    
    if user_input:
        # Check if we need more context about the user's goals
        if "longevity" in user_input.lower() or "health" in user_input.lower():
            interview_notes.append("Consider asking about specific health goals, current lifestyle, and any existing conditions.")
        
        # Check if we need more specific information
        if len(user_input.split()) < 5:
            interview_notes.append("User input is brief - may need clarification on specific areas of interest.")
    
    # Check conversation depth
    if len(messages) < 2:
        interview_notes.append("Limited conversation history - consider building rapport and understanding context.")
    
    # Generate interview summary
    if interview_notes:
        interview_summary = "Interview Analysis: " + " ".join(interview_notes)
    else:
        interview_summary = "Interview Analysis: Sufficient information provided. Ready to proceed with recommendations."
    
    # Return updated state
    # In a real system, this might add follow-up questions to the messages or set a flag
    return {
        "interviewSummary": interview_summary
    }


def guardrailsNode(state: GraphState) -> GraphState:
    """
    Node that applies guardrails to ensure safety and appropriateness.
    
    This node checks the user input, suggestions, and responses for:
    - Medical safety concerns
    - Inappropriate content
    - Compliance with regulations
    - Ethical considerations
    
    Args:
        state: The current graph state
        
    Returns:
        Updated state with guardrails check results
    """
    # Extract data to check
    user_input = state.get("userInput", "")
    suggestion = state.get("suggestion", "")
    response = state.get("response", "")
    
    # Initialize guardrails results
    warnings = []
    safe = True
    
    # Check for medical disclaimer needs
    medical_keywords = ["diagnose", "treatment", "cure", "medication", "prescription"]
    if any(keyword in user_input.lower() for keyword in medical_keywords):
        warnings.append("Medical disclaimer: This information is for educational purposes only and not medical advice.")
    
    if any(keyword in suggestion.lower() for keyword in medical_keywords):
        warnings.append("Suggestion contains medical information - ensure proper disclaimers are included.")
    
    # Check for prohibited content
    prohibited_keywords = ["illegal", "harmful", "dangerous"]
    if any(keyword in user_input.lower() for keyword in prohibited_keywords):
        safe = False
        warnings.append("Warning: Input may contain prohibited content. Review required.")
    
    # Check for age-related concerns
    if "child" in user_input.lower() or "minor" in user_input.lower():
        warnings.append("Note: Content involves minors - ensure age-appropriate recommendations.")
    
    # Generate guardrails summary
    if safe:
        if warnings:
            guardrails_status = f"PASSED WITH WARNINGS: {'; '.join(warnings)}"
        else:
            guardrails_status = "PASSED: All safety checks passed. Content is appropriate."
    else:
        guardrails_status = f"BLOCKED: {'; '.join(warnings)}"
    
    # Return updated state
    return {
        "guardrailsStatus": guardrails_status,
        "guardrailsSafe": safe
    }


def create_graph():
    """
    Creates and compiles the LangGraph with all nodes.
    
    Returns:
        Compiled LangGraph ready for execution
    """
    # Create a new StateGraph with GraphState as the state schema
    workflow = StateGraph(GraphState)
    
    # Add nodes to the graph
    workflow.add_node("interviewNode", interviewNode)
    workflow.add_node("guardrailsNode", guardrailsNode)
    workflow.add_node("suggestion", suggestion_node)
    workflow.add_node("critic", critic_node)
    
    # Set the entry point
    workflow.set_entry_point("suggestion")
    
    # Define the edge from suggestion to critic
    workflow.add_edge("suggestion", "critic")
    
    # Add edge from critic to END
    workflow.add_edge("critic", END)
    
    # Compile the graph
    app = workflow.compile()
    
    return app


def main():
    """
    Main execution function for the LangGraph application.
    
    Creates the graph, initializes it with an empty state,
    executes it, and displays the results.
    """
    print("Creating LangGraph...")
    app = create_graph()
    
    print("Initializing graph with empty state...")
    initial_state: GraphState = {}
    
    print("Executing graph...")
    print("-" * 60)
    
    # Execute the graph
    final_state = app.invoke(initial_state)
    
    print("-" * 60)
    print("\nExecution complete!\n")
    print("Results:")
    print("-" * 60)
    
    if "suggestion" in final_state:
        print(f"Suggestion: {final_state['suggestion']}\n")
    else:
        print("Suggestion: (not generated)\n")
    
    if "critique" in final_state:
        print(f"Critique: {final_state['critique']}\n")
    else:
        print("Critique: (not generated)\n")
    
    print("-" * 60)
    
    return final_state


if __name__ == "__main__":
    main()

